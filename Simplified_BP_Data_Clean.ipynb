{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Simplified BP_Data_Clean.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/BP_PPG/blob/master/Simplified_BP_Data_Clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH7Qe1yuCa3X",
        "colab_type": "text"
      },
      "source": [
        "Run all cells to generate cleaned data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_ZSkQAudJT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import wfdb\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "import io\n",
        "import pickle\n",
        "import numba\n",
        "from numba import jit\n",
        "import tensorflow as tf\n",
        "from scipy.signal import find_peaks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRgQ3jT1CIeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jit(nopython=True)\n",
        "def flat_line(signals,threshold = 0, percent = .15):\n",
        "  clean_signals = []\n",
        "  #create a list to store the index of the removed segments, this will be used\n",
        "  #to remove the PPG signals with same index\n",
        "  rm_list = []\n",
        "  for i in range(len(signals)):\n",
        "    #use np.diff to find consecutive points: diff = [i] - [i+1]\n",
        "    signal_diff = np.diff(signals[i])\n",
        "    #change value less than threshold to 0, and the rest to 1\n",
        "    less = np.abs(signal_diff) <= threshold\n",
        "    more = np.abs(signal_diff) > threshold\n",
        "    signal_diff[less] = 0\n",
        "    signal_diff[more] = 1\n",
        "    #calculate what percent of 0 in the signal, remove the entire signal if \n",
        "    #percentage is higher than defined percent\n",
        "    zero_per = np.sum(signal_diff==0)/len(signal_diff)\n",
        "    if zero_per < percent:\n",
        "      clean_signals.append(signals[i])\n",
        "    else:\n",
        "      rm_list.append(i)\n",
        "    \n",
        "    #track the progress for impatient programmer\n",
        "    #if i%10000 == 0:\n",
        "      #print(\"Processing on\", i, \"th sample\")\n",
        "\n",
        "  return clean_signals,rm_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCFauiD-DMSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_segment_data(source,seg_len):\n",
        "  signals =[]\n",
        "  for signal in source:\n",
        "    for i in range(int(len(signal)/seg_len)):\n",
        "      seg = signal[seg_len*i:seg_len*(i+1)]\n",
        "      signals.append(seg)\n",
        "#convert list into a numpy array and change its dim from (num of records, seg_len, 1) to (num of records, seg_len)\n",
        "  signals = np.asarray(list(map(lambda x: np.reshape(x,7500),signals)))\n",
        "\n",
        "  return signals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynj6-Qk6AKsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def peak_segmentation(signal,distance = 60):\n",
        "  valleys, _ = find_peaks(signal*-1, distance=distance)\n",
        "  \n",
        "  segments = []\n",
        "  for i in range(len(valleys)-1):\n",
        "    seg = signal[valleys[i]:valleys[i+1]]\n",
        "    segments.append(seg)\n",
        "  \n",
        "  return segments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CA3vq1Ol65w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#flat_peak: remove cycles that has flat peak in a 1min signal\n",
        "#input: cycles: 1 min signals that consisits many heart beat cycles\n",
        "#       tolerance: number of acceptable flat point \n",
        "#output: cleaned 1 min signals by removing flat peak cycles\n",
        "def flat_peak(cycles, tolerance = 2):\n",
        "  \n",
        "  clean_cycles = []\n",
        "\n",
        "  for i in range(len(cycles)):\n",
        "    if len(np.argwhere(cycles[i] == np.amax(cycles[i]))) > tolerance:\n",
        "      continue;\n",
        "    else: \n",
        "      clean_cycles.append(cycles[i])\n",
        "\n",
        "  return clean_cycles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTTz-EoJNfgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#flat_peak_remove:remove 1min segements that has flat peak\n",
        "#input: signals: A list of list (i.e. a list of 1 min signals that contains cyclic segments: ABP_ps_signals)\n",
        "#       seg_ratio: threhold to remove the whole 1min signal (i.e. if the ratio of \n",
        "#                   \"No. of cleaned flat peak cycles\"/\"No. of original 1min cycles\" \n",
        "#                   is less than this treshold, then the whole 1min signal of cycles will be removed )\n",
        "#output: cleaned_segments: a list of cleaned 1min signals\n",
        "#        remova_index: a list of removed 1min signals\n",
        "def flat_peak_remove(signals, seg_ratio = 0.95, tolerance = 2):\n",
        "\n",
        "  clean_segments = []\n",
        "  remove_index = [] \n",
        "\n",
        "  for i in range(len(signals)):\n",
        "    #in case some lists are empty\n",
        "    if signals[i] == []: \n",
        "      remove_index.append(i) \n",
        "      continue  \n",
        "\n",
        "    clean_cycles = flat_peak(signals[i],tolerance)\n",
        "    \n",
        "    if len(clean_cycles)/(len(signals[i])) < seg_ratio: \n",
        "      remove_index.append(i)\n",
        "      continue\n",
        "    \n",
        "    clean_segments.append(clean_cycles)\n",
        "\n",
        "  return clean_segments, remove_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRxBSD1-fhu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bp_ground_truth: A function that returns average systolic and diastolic value \n",
        "#                  of a 1min data\n",
        "#Inputs: signals: ABP_fpr_signals\n",
        "#      tolerance: acceptable fluctuations (max - min) of either systolic and diastolic \\\n",
        "#                 during a 1min measurement\n",
        "#outputs: gt_ls: a list of ground truth for a list of 1min signals\n",
        "#         remove_index: a list of 1 min signals that has either systolic or diastolic \\\n",
        "#                       fluctuation > tolerance\n",
        "def bp_ground_truth(signals, tolerance = 20):\n",
        "  gt_ls = []\n",
        "  remove_index = []\n",
        "  for i in range(len(signals)):\n",
        "    cycles = signals[i]    #a list of cycles in 1min signal\n",
        "    \n",
        "    cyc_sys_list = []\n",
        "    cyc_dia_list = []\n",
        "    for j in range(len(cycles)):\n",
        "      cyc_sys_list.append(max(cycles[j]))\n",
        "      cyc_dia_list.append(cycles[j][0])\n",
        "\n",
        "    if np.max(np.asarray(cyc_sys_list)) - np.min(np.asarray(cyc_sys_list)) > tolerance \\\n",
        "    or np.max(np.asarray(cyc_dia_list)) - np.min(np.asarray(cyc_dia_list)) > tolerance:\n",
        "      remove_index.append(i)\n",
        "      continue\n",
        "\n",
        "    else: gt_ls.append([np.average(np.asarray(cyc_sys_list)),\n",
        "                  np.average(np.asarray(cyc_dia_list))])\n",
        "\n",
        "  return gt_ls, remove_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zfB0Sk6ricU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.signal import butter, lfilter\n",
        "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc14FBigPpsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#use numba to improve the speed of for loop\n",
        "@jit(nopython=True)\n",
        "def hampel_filter_forloop_numba(input_series, window_size, n_sigmas=3):\n",
        "    \n",
        "    n = len(input_series)\n",
        "    new_series = input_series.copy()\n",
        "    k = 1.4826 # scale factor for Gaussian distribution\n",
        "    #indices = []\n",
        "    \n",
        "    for i in range((window_size),(n - window_size)):\n",
        "        x0 = np.nanmedian(input_series[(i - window_size):(i + window_size)])\n",
        "        S0 = k * np.nanmedian(np.abs(input_series[(i - window_size):(i + window_size)] - x0))\n",
        "        if (np.abs(input_series[i] - x0) > n_sigmas * S0):\n",
        "            new_series[i] = x0\n",
        "            #indices.append(i)\n",
        "    \n",
        "    return new_series#, indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I17_KbRBaQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_data(directory):\n",
        "  #load data\n",
        "  ABP_names = glob.glob(directory + \"ABP*.pkl\")\n",
        "  PPG_names = glob.glob(directory + \"PPG*.pkl\")\n",
        "  assert(len(ABP_names) == len(PPG_names))\n",
        "  for i in range(len(ABP_names)):\n",
        "    \n",
        "    print(\"processing\", i, \"th data\")\n",
        "\n",
        "    with open(ABP_names[i], \"rb\") as fp:\n",
        "      ABP_raw_signals = pickle.load(fp)\n",
        "\n",
        "    with open(PPG_names[i], \"rb\") as fp:\n",
        "      PPG_raw_signals = pickle.load(fp)\n",
        "\n",
        "    #remove flat lines on record level: ABP: 10%, PPG: 10%\n",
        "    _, PPG_rm_list = flat_line(PPG_raw_signals,0,percent=0.10)\n",
        "    _, ABP_rm_list = flat_line(ABP_raw_signals,0,percent=0.10)\n",
        "    \n",
        "    ABP_list = pd.DataFrame(ABP_rm_list)\n",
        "    PPG_list = pd.DataFrame(PPG_rm_list)\n",
        "\n",
        "    try:\n",
        "      total_list = ABP_list.merge(PPG_list,how=\"outer\")\n",
        "    except:\n",
        "      total_list = pd.concat([ABP_list,PPG_list],axis=0)\n",
        "      \n",
        "    removal_list=total_list.values.tolist()\n",
        "    ABP_cl_signals = np.delete(ABP_raw_signals,total_list,0)\n",
        "    PPG_cl_signals = np.delete(PPG_raw_signals,total_list,0)  \n",
        "\n",
        "    #segment into 1min data\n",
        "    ABP_seg_signals = generate_segment_data(ABP_cl_signals, 7500)\n",
        "    PPG_seg_signals = generate_segment_data(PPG_cl_signals, 7500)\n",
        "\n",
        "    #remove flat lines on 1min segment level: ABP: 2%, PPG: 5%\n",
        "    _, ABP_seg_rm_list = flat_line(ABP_seg_signals,0,percent=0.02)\n",
        "    _, PPG_seg_rm_list = flat_line(PPG_seg_signals,0,percent=0.05)\n",
        "\n",
        "    ABP_seg_list = pd.DataFrame(ABP_seg_rm_list)\n",
        "    PPG_seg_list = pd.DataFrame(PPG_seg_rm_list)\n",
        "\n",
        "    try:\n",
        "      total_seg_list = ABP_seg_list.merge(PPG_seg_list,how=\"outer\")\n",
        "    except:\n",
        "      total_seg_list = pd.concat([ABP_seg_list,PPG_seg_list],axis=0)\n",
        "    \n",
        "    removal_list=total_list.values.tolist()\n",
        "    ABP_seg_cl_signals = np.delete(ABP_seg_signals,total_seg_list,0)\n",
        "    PPG_seg_cl_signals = np.delete(PPG_seg_signals,total_seg_list,0)\n",
        "\n",
        "    #PROCESS ABP SIGNAL\n",
        "    #1.peak segmentation: distance = 60\n",
        "    ABP_ps_signals = [peak_segmentation(i, distance = 60) for i in ABP_seg_cl_signals]\n",
        "    #2.flat peak removal: seg_ratio = 0.95, tolerance = 2\n",
        "    ABP_fpr_signals, remove_index = flat_peak_remove(ABP_ps_signals,0.95,2)\n",
        "    #3.remove corresponding PPG signal\n",
        "    PPG_fpr_signals = np.delete(PPG_seg_cl_signals,remove_index,0)\n",
        "    #4.generate ground truth ABP: tolerance = 30 mmHg\n",
        "    gt_ls, gt_rm_list= bp_ground_truth(ABP_fpr_signals, tolerance=20)\n",
        "    #5.remove corresponding PPG signals\n",
        "    PPG_gt_signals = np.delete(PPG_fpr_signals,gt_rm_list,0)\n",
        "\n",
        "    #PROCESS PPG SIGNAL\n",
        "    #1.standardize PPG signal\n",
        "    PPG_norm_signals = [sklearn.preprocessing.robust_scale(i) for i in PPG_gt_signals]\n",
        "    #2.band pass filter on PPG sinal\n",
        "    PPG_bf_signals =[butter_bandpass_filter(i,0.5,8,125,order=4) for i in PPG_norm_signals]\n",
        "    #3. hampel filter\n",
        "    PPG_hf_signals = [hampel_filter_forloop_numba(i, 6) for i in PPG_bf_signals]\n",
        "    #4. resample signal\n",
        "    ##PLACEHOLDER for resampling signal to a lower frequency, if needed\n",
        "    with open(directory + \"BP_data\" + \"_\" + str(i), \"wb\") as fp:\n",
        "      pickle.dump(PPG_hf_signals,fp)\n",
        "\n",
        "    with open(directory + \"BP_label\" + \"_\" + str(i), \"wb\") as fp:\n",
        "      pickle.dump(gt_ls,fp)\n",
        "\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvxOyz5DD3zN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "directory = 'D:/WFDB//matched/BP/Original Data/'\n",
        "process_data(directory)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}